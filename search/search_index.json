{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-skele-friend","title":"Welcome Skele-Friend! \ud83d\udc80\u2728","text":"<p>This is the official and most up-to-date place to find documentation for the FreeMoCap project.</p> <p>Our documentation is a continuous work in progress, so we appreciate your patience, support, and engagement!</p> <p>Quick start: Take me to the skeletons!\"</p> <p>Head on over to our Tutorials section and work through the Single-Camera Recording walkthrough tp get started using FreeMoCap!</p>"},{"location":"#helpful-links","title":"Helpful Links","text":"<ul> <li>The FreeMoCap Website https://freemocap.org</li> <li>The FreeMoCap GitHub https://github.com/freemocap/freemocap</li> <li>Support FreeMoCap by donating to our non-profit that supports our work!</li> </ul>"},{"location":"#troubleshooting","title":"Troubleshooting?","text":"<ul> <li>If you run into an issue using the software itself, post an issue on our GitHub, here: https://github.com/freemocap/freemocap/issues </li> <li> <p>If there's an error in our documentation, post an issue on our documentation repository, here: https://github.com/freemocap/documentation/issues</p> </li> <li> <p>Join the Discord and ask a question in the #help-requests channel.</p> <ul> <li>Click here to join our Discord</li> </ul> </li> </ul>"},{"location":"about_us/","title":"About Us","text":"<p>The Free Motion Capture Project (FreeMoCap) aims to provide research-grade markerless motion capture software to everyone for free.</p> <p>We're building a user-friendly framework that connects an array of bleeding edge open-source tools from the computer vision and machine learning communities to accurately record full-body 3D movement of humans, animals, robots, and other objects.</p> <p>We want to make the newly emerging mind-boggling, future-shaping technologies that drive FreeMoCap's core functionality accessible to communities of people who stand to benefit from them.</p> <p>We follow a \u201cUniversal Design\u201d development philosophy, with the goal of creating a system that serves the needs of a professional research scientist while remaining intuitive to a 13-year-old with no technical training and no outside assistance.</p> <p>A high-quality, minimal-cost motion capture system would be a transformative tool for a wide range of communities - including 3d animators, game designers, athletes, coaches, performers, scientists, engineers, clinicians, and doctors. We hope to create a system that brings new technological capacity to these groups while also building bridges between them.</p> <p></p> Everyone has a reason to record human movement <p>We want to help them do it</p> <p>\u2728\ud83d\udc80\u2728 </p> <p></p> <p>This project is managed by the FreeMoCap Foundation</p>"},{"location":"about_us/#software-overview","title":"Software Overview","text":"<p>FreeMoCap (free motion capture) is a free open source  markerless motion capture system designed to provide research-quality motion capture data using free software and generic, minimal-cost webcams. The data it provides can be useful for any project that would benefit from high quality 3d measurments of human movement, including scientific research, 3D animation, sports biomechanics, and more.</p>"},{"location":"about_us/#features-and-capabilities","title":"Features and Capabilities","text":"<p>FreeMoCap features a complete GUI-based interface that can create high-quality kinematic data from single cameras, multiple cameras, or imported videos. It also produces data outputs in the form of numpy arrays, CSVs, a Blender output scene, and a preloaded Jupyter notebook that is set up for analyzing the data that was just produced. This is especially useful for classroom settings or immediate opportunities for exploratory data analysis the moment the data is done processing. The software is designed to work with minimal-cost, low-quality USB webcams, as well as asynchronous recording methods such as GoPros. Support for research-grade cameras like FLIR or IP cameras like White Matter is planned.</p>"},{"location":"about_us/#community-involvement-and-support","title":"Community Involvement and Support","text":"<p>FreeMoCap has a vibrant and growing community of users and developers, including research and clinical scientists, 3D animators, video game designers, and open-source software developers. Most of the community is centered around a Discord server.</p> <p>Here, people can ask questions and receive support from the developers and other members of the community. Feature requests and bug reports should be submitted to the GitHub issues space.</p>"},{"location":"community/code_of_conduct/","title":"Code of Conduct","text":""},{"location":"community/code_of_conduct/#code-of-conduct","title":"Code of Conduct","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to make participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"community/code_of_conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of inappropriately sexualized language or imagery and unwelcome sexual attention or advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a professional setting</li> </ul>"},{"location":"community/code_of_conduct/#conflict-resolution","title":"Conflict Resolution","text":"<p>If conflicts arise within the community, we encourage participants to resolve them through respectful dialogue. If necessary, contact us at info at freemocap dot org and we will do our best to facilitate resolution by providing guidance and mediation.</p>"},{"location":"community/privacy_policy/","title":"Privacy Policy","text":""},{"location":"community/privacy_policy/#privacy-policy","title":"Privacy Policy","text":"<p>We respect and value your privacy. This Privacy Policy outlines how we collect, use, and protect any personal or anonymous data we may collect from users of our software. By using our software, you agree to the terms of this Privacy Policy. \"We\" refers to the FreeMoCap development team, which maintains the FreeMoCap software. \"You\" refers to the user of our software.</p> <p>User data helps us understand how we can make our software better for you and allows us to demonstrate to agencies and corporations that people are using our software, which may help us to grow the project in the future.</p>"},{"location":"community/privacy_policy/#collection-of-anonymous-user-data","title":"Collection of Anonymous User Data","text":"<p>If you check the \"Send anonymous usage information\" check box on the main page of the GUI, we will collect anonymous user data when you use our software. This information is sent to us as a \"ping\" of data posted to pipedream every time the software is used. </p> <p>The data we currently collect includes:</p> <ul> <li>your IP Address (because the \"ping\" is sent to pipedream via a POST request, we cannot avoid collecting your IP address)</li> <li>The time the \"ping\" was sent </li> <li>information about your cameras</li> </ul> <p>You can view the code that collects this data here.</p> <p>If you do not wish to share your data, you may turn off \"pings\" at any point in time. To turn off user pings, uncheck the \"Send anonymous usage information\" box on the home screen of the FreeMoCap GUI.</p>"},{"location":"community/privacy_policy/#protection-of-user-data","title":"Protection of User Data","text":"<p>We take the protection of your data seriously and will not sell or distribute any personal or anonymous data we collect to third parties, except as required by law. We use industry-standard security measures to protect your data from unauthorized access, use, or disclosure. Currently, only core FreeMoCap developers (Jonathan Matthis, and Trenton Wirth) have access to the user data we have collected.</p>"},{"location":"community/privacy_policy/#your-control-over-your-data","title":"Your Control Over Your Data","text":"<p>As a user of our software, you have control over your data. You can choose to turn off \"pings\" at any point in time by unchecking the \"Send anonymous usage information\" box on the home screen of the FreeMoCap GUI.</p> <p>If you wish to have your user data deleted, you may contact us at info AT freemocap DOT org.</p>"},{"location":"community/privacy_policy/#updates-to-this-privacy-policy","title":"Updates to This Privacy Policy","text":"<p>We may update this Privacy Policy as our project evolves, so please check back periodically for changes. Your continued use of our software after any changes to this Privacy Policy will constitute your acceptance of such changes.</p>"},{"location":"community/privacy_policy/#contact-us","title":"Contact Us","text":"<p>If you have any questions or concerns about this Privacy Policy or our use of your data, please contact us at info AT freemocap DOT org, or reach out to us on our Discord.</p>"},{"location":"contributing/","title":"Contributing Guide","text":""},{"location":"contributing/#contributing-to-the-freemocap-project","title":"Contributing to the FreeMoCap Project","text":"<p>Welcome to the FreeMoCap contributing guide! </p> <p>This document will help you understand how to contribute to the project, whether you're reporting bugs, suggesting new features, or submitting code changes. We're excited to have you on board and look forward to working with you!</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":""},{"location":"contributing/#installing-freemocap-from-source-code","title":"Installing FreeMoCap from source code","text":""},{"location":"contributing/#reporting-bugs","title":"Reporting Bugs","text":"<p>Your feedback is invaluable in improving FreeMoCap. In this section, you'll learn how to report bugs and suggest new features for the project.</p> <p>For details, see our Bug Report guide</p>"},{"location":"contributing/#suggesting-features","title":"Suggesting Features","text":"<p>We welcome your ideas for new features or improvements to FreeMoCap. To suggest a feature, open a new issue and describe the feature, its benefits, and any potential challenges in implementing it.</p> <p>For details, see our Feature Request  guide</p>"},{"location":"contributing/#code-contributions","title":"Code Contributions","text":"<p>Contributing code to FreeMoCap involves creating and submitting pull requests. In this section, you'll learn about our development process, coding styles, and testing requirements.</p>"},{"location":"contributing/#we-use-github-flow","title":"We Use GitHub Flow","text":"<p>We use Github Flow as our development process. It makes it easy for contributors to submit changes and maintainers to review and merge them.</p>"},{"location":"contributing/#code-changes-happen-through-pull-requests","title":"Code Changes Happen Through Pull Requests","text":"<p>Pull requests are the best way to propose changes to the codebase (we use Github Flow). </p> <p>We actively welcome your pull requests: 1. Fork the repo and create your branch from <code>main</code>. 2. If you've added code that should be tested, add tests. 3. If you've changed APIs, update the documentation. 4. Ensure the test suite passes. 5. Make sure your code lints. 6. Issue that pull request!</p>"},{"location":"contributing/#coding-styles","title":"Coding Styles","text":"<p>Consistent coding styles are important for maintaining a clean and easy-to-understand codebase. We use the following style guides for our project:</p> <ul> <li>Python Code Style Guide</li> <li>PyQT: [COMING SOON]</li> <li>API: [COMING SOON]</li> </ul>"},{"location":"contributing/#testing","title":"Testing","text":"<p>We require tests for all code contributions to ensure the stability and reliability of the project. When submitting a pull request, make sure your code is covered by tests, and that those tests pass our Github Actions workflow.</p> <p>Here's a nice introduction to testing in Python.</p>"},{"location":"contributing/#pull-requests","title":"Pull Requests","text":"<p>Pull requests are the best way to propose changes to the codebase (we use Github Flow). We actively welcome your pull requests:</p> <ol> <li>Fork the repo and create your branch from <code>main</code>.</li> <li>If you've added code that should be tested, add tests.</li> <li>If you've changed APIs, update the documentation.</li> <li>Ensure the test suite passes.</li> <li>Make sure your code lints.</li> <li>Issue that pull request!</li> </ol>"},{"location":"contributing/#getting-help-and-asking-questions","title":"Getting Help  and Asking Questions","text":"<p>If you encounter issues or have questions - ask for help on our Discord server or by creating a new issue on Github. We'll do our best to assist you and provide the information you need.</p>"},{"location":"contributing/bug_report/","title":"Bug report","text":""},{"location":"contributing/bug_report/#write-bug-reports-with-detail-background-and-sample-code","title":"Write bug reports with detail, background, and sample code","text":"<p>This is an example of a \"great\" bug report.</p> <p>Great Bug Reports tend to have:</p> <ul> <li>A quick summary and/or background</li> <li>Steps to reproduce</li> <li>Be specific!</li> <li>Include an uploaded ZIP of the freemocap data session you ran that produced the issue.</li> <li>Give sample code if you can. This stackoverflow question demonstrates the user giving as much information as possible.</li> <li>What you expected would happen</li> <li>What actually happens</li> <li>Notes (possibly including why you think this might be happening, or stuff you tried that didn't work)</li> </ul> <p>People love thorough bug reports. The likelihood that the community and/or maintainers will help you with your issues stems from how well you can help them understand you. :)</p>"},{"location":"contributing/feature_request/","title":"Feature request","text":"<p>We welcome new ideas and feature requests from the community! To propose a new feature or enhancement, please open a new issue with the following details:</p> <ul> <li>A clear and concise description of the feature</li> <li>The problem you're trying to solve or the goal you want to achieve</li> <li>If possible, provide examples or mockups to illustrate the desired functionality</li> <li>Any alternatives or workarounds you've considered, if applicable</li> </ul> <p>Please be patient and understand that we might not be able to implement every requested feature, but we will consider each request and prioritize them based on the needs of the project and the community.</p>"},{"location":"contributing/python_code_style_guide/","title":"Python Style Guide","text":"<p>This style guide aims to maintain code readability, quality, and maintainability. </p>"},{"location":"contributing/python_code_style_guide/#general-guidelines","title":"General Guidelines","text":"<ul> <li>Follow a Universal Design approach that aspires to be interpretable to the widest possible number of people </li> <li>Follow  PEP 8 Guidelines</li> <li>Follow standard best practices, e.g S.O.L.I.D, etc</li> <li>Use  Black to auto-format your code</li> </ul>"},{"location":"contributing/python_code_style_guide/#specific-guidelines","title":"Specific Guidelines","text":"<ol> <li> <p>Include Google-formatted docstrings: Use Google-style docstrings for functions, methods, and classes to provide    clear and concise documentation.  </p> </li> <li> <p>Type hints: Use input and return type hints for functions and methods to improve code readability and facilitate    better tooling support.  </p> </li> <li> <p>Keyword arguments: Prefer using keyword arguments over simple arguments for functions and methods to improve code    clarity.  </p> </li> <li> <p>Private methods and attributes: Use leading underscores to denote private methods and attributes in classes, and    use <code>@property</code> decorators when appropriate.  </p> </li> <li> <p>Descriptive names: Use full words in variable and class names instead of abbreviations (e.g., <code>database</code> instead   of <code>db</code>).  </p> </li> <li> <p>PEP8 and <code>black</code> formatting: Follow PEP8 and <code>black</code> code formatting guidelines to maintain consistency and  readability.  </p> </li> <li> <p>Consistent naming conventions: Adopt consistent naming conventions for variables, functions, and classes.  </p> </li> <li>Use <code>snake_case</code> for variables and functions (e.g., <code>my_variable</code>, <code>my_function</code>)  </li> <li>Use <code>PascalCase</code> for class names (e.g., <code>MyClass</code>)  </li> <li> <p>Use <code>UPPERCASE</code> for constants (e.g., <code>MY_CONSTANT</code>)  </p> </li> <li> <p>Keep functions and methods short: Aim to keep functions and methods concise, ideally not exceeding 15-20 lines of  code.  </p> </li> <li> <p>Modularize code: Organize code into modules and packages to maintain a clean and organized codebase.  </p> </li> <li> <p>Minimal comments: Avoid comments if possible. Write code that is simple and descriptive (See pt 5) enough that comments are  not necessary. If needed, use comments sparingly to provide context or explain complex or non-obvious sections of  your code.  </p> </li> <li> <p>Error handling: Use appropriate error handling techniques, such as <code>try</code> and <code>except</code> blocks, to handle     exceptions and provide meaningful error messages to users.  </p> </li> <li> <p>Write tests: Write unit tests to ensure the correct functioning of your code.  </p> </li> <li> <p>Code reviews: Perform code reviews with team members or peers to maintain a high-quality codebase.</p> </li> </ol>"},{"location":"getting_started/","title":"Welcome","text":"<p>Thanks for your interest in using the FreeMoCap software! We're excited to have you here, and we're looking forward to seeing what you create with this tool  </p> <p>FreeMoCap runs on Windows, Mac (ARM and Intel), and Linux operating systems.</p> <p>Follow the instructions below, starting with the environment setup and package installation steps and you'll be up and running in no time! </p> <p>Head on over to our Discord and let us know if you run into any issues getting FreeMoCap running on your machine. We're always working to improve cross-platform compatibility and appreciate your feedback. </p> <p>Now let's get started capturing some motion!</p> <p>First step   Install the software</p>"},{"location":"getting_started/#basic-freemocap-workflow","title":"Basic FreeMoCap Workflow","text":"<pre><code>graph TD\n    installation(Installation)\n    setup(Setup)\n    calibrate(Calibrate Capture Volume)  \n    record_videos(Record/Process Motion Capture Videos)    \n    visualize(Visualization)\n    analysis(Analysis)\n\n    installation--&gt;setup\n    setup --if single camera---&gt; record_videos\n    setup -- if multiple cameras ---&gt; calibrate    \n    calibrate --&gt; record_videos    \n\n    record_videos --&gt; visualize\n    record_videos --repeat---&gt; analysis\n\n    visualize --&gt; analysis\n    visualize --&gt; record_videos\n\n    analysis --&gt;  visualize\n    analysis --forever---&gt; record_videos</code></pre>"},{"location":"getting_started/installation/","title":"Installation","text":"<p>QUICKSTART</p> <p>If you're already familiar with Python environments and package installation, you can get started with FreeMoCap by simply:</p>"},{"location":"getting_started/installation/#1-create-a-a-python-39-through-311-environment-python311-recommended","title":"1. Create a a Python 3.9 through 3.11 environment (<code>python3.11</code> recommended)","text":""},{"location":"getting_started/installation/#2-enter-command-pip-install-freemocap","title":"2. Enter command: <code>pip install freemocap</code>","text":""},{"location":"getting_started/installation/#3-enter-command-freemocap","title":"3. Enter command: <code>freemocap</code>","text":"<p>...and you're off to the races!</p>"},{"location":"getting_started/installation/#detailed-installation-instructions","title":"Detailed Installation Instructions:","text":""},{"location":"getting_started/installation/#0-install-anaconda-or-miniconda-if-you-havent-already","title":"0.  Install Anaconda (or Miniconda) if you haven't already.","text":"<p>Python Environments</p> <p>If you are not familiar with Python environments, here's a nice guide from Real Python.</p> <p>We recomend low XP folk use <code>conda</code> to manage their python environment because its the easiest to set up, but you can use whatever method you like.</p> <p>Personally, I use <code>poetry</code> </p>"},{"location":"getting_started/installation/#1-open-a-terminal-window","title":"1. Open a terminal window","text":"WindowsMacLinux <p>Press the <code>Windows key</code>, type \"Anaconda Prompt\", and press Enter.</p> <p>Press <code>command + spacebar</code> and type \"terminal\" and press Enter. </p> <p>Press <code>ctrl + alt + t</code> to open a terminal window.</p>"},{"location":"getting_started/installation/#2-create-a-new-python-environment","title":"2. Create a new Python environment","text":"<ul> <li>Create a new Python environment (verions 3.9 through 3.11 recommended). To do so, enter the following command in your terminal:  <pre><code>conda create -n freemocap-env python=3.11 -y\n</code></pre> </li> </ul> Breakdown of the <code>conda create</code> command <p>This step creates a new installation of Python on your system that will be used to run the code behind the FreeMoCap software!</p> <p>Here's a breakdown of what's happening in this command:</p> <p>conda create -n freemocap-env python=3.11 -y`</p> <ul> <li> <p><code>conda</code>:    The <code>conda</code> package manager (i.e. the program that will execute the command that we send it)</p> </li> <li> <p><code>create</code>:    The command to create a new environment</p> </li> <li> <p><code>-n freemocap-env</code>:  Set the name of the new environment to <code>freemocap-env</code>(note: <code>-n</code> is short for <code>--name</code>)</p> </li> <li> <p><code>python=3.11</code>:    Install Python version 3.11 into the new environment</p> </li> <li> <p><code>-y</code>:    Automatically answer \"yes\" to any prompts that come up during the installation process</p> </li> </ul> <p>So, all together, this command is saying:</p> <p>\"Hey, Conda! Please create a new environment named <code>freemocap-env</code> and install Python version 3.11 into it. If you need to ask me any questions during the process, just answer 'Yes' for me.\"</p> <p>Enter <code>conda --help</code> or <code>conda create --help</code> into your terminal to learn more about the <code>conda</code> command and its options.</p> <p>For more information about Python environments, check out this guide from Real Python</p> <ul> <li>Activate that newly created environment <pre><code>conda activate freemocap-env\n</code></pre> </li> </ul>"},{"location":"getting_started/installation/#3-install-software","title":"3. Install software","text":"PyPi (pip)Github (source code) <pre><code>pip install freemocap\n</code></pre> <pre><code>git clone https://github.com/freemocap/freemocap\ncd freemocap\npip install -e .    \n</code></pre>"},{"location":"getting_started/installation/#4-launch-the-gui","title":"4. Launch the GUI","text":"<p>To launch FreeMoCap, enter the command <code>freemocap</code> into the terminal, like this: </p><pre><code>freemocap\n</code></pre> Put together, steps 3 &amp; 4 should look like this in your terminal:  <p>Once <code>freemocap</code> is entered into your terminal, a GUI should pop up that looks something like this:</p> <p></p>"},{"location":"getting_started/installation/#congrats-youre-in","title":"Congrats, you're in!","text":"<p>Now that you've got FreeMoCap installed, you're ready to record your first motion capture session!</p> <p>The first step is to set up your environment for motion capture and connect your cameras</p> <p> Set up your environment</p>"},{"location":"getting_started/multi_camera_calibration/","title":"Multi-Camera Calibration Tutorial","text":""},{"location":"getting_started/multi_camera_calibration/#multi-camera-calibration-guide","title":"Multi-Camera Calibration Guide","text":"<p>Tip</p> <p>Check out this video for more information and directed guidance in the calibration process</p> <p>Note: This calibration process describes the use of an anipose-based calibration method. We will soon be updating our method to use a more flexible and interactive interface.</p>"},{"location":"getting_started/multi_camera_calibration/#preparing-the-charuco-board","title":"Preparing the Charuco Board","text":"<p>To perform a multi-camera calibration, you'll need to print out a Charuco board image. </p> <p>For smaller spaces, a simple printout from a standard printer should work just fine. Make sure to mount the printout on something rigid like cardboard - the calibration process requires a flat charuco board.</p> <p>For larger spaces, you might need to print this on a larger poster board so that it can be seen well by the cameras.</p>"},{"location":"getting_started/multi_camera_calibration/#recording-calibration-videos","title":"Recording Calibration Videos","text":"<p>In the camera view section of the GUI, select the calibration videos option: </p> <p>Begin the recording, and then move until your Charuco board can be seen in the overlapping fields of view of at least two cameras at a time. Move the Charuco board up and down so that you are \"painting\" each camera's view with images of the board. Make sure that every camera has shared views of the board with at least one other camera. We will be using the corresponding views of the board with the other cameras to help localize the camera positions relative to each other, which is necessary for the 3D triangulation step later.</p> <p>For more information about how to use the board to get a high quality calibration, see this video (it uses a different version of this software, but the same principles apply).</p>"},{"location":"getting_started/multi_camera_calibration/#processing-the-calibration","title":"Processing the Calibration","text":"<p>Once you have given each camera a good view of the board shared with another camera, click \"Stop Recording,\" and it will begin the calibration process automatically. </p> <p>Tip</p> <p>Be sure to keep an eye on the terminal that launched the GUI for helpful output, as the log at the bottom of the  GUI screen does not capture all of the outputs yet.</p>"},{"location":"getting_started/multi_camera_calibration/#recording-motion-capture-videos","title":"Recording Motion Capture Videos","text":"<p>Once you have completed the calibration process, you are are ready to record motion capture videos!</p> <p>Select \"Motion Capture Videos\" from the camera view section of the GUI, and then click \"Record.\" Perform your movement, and then click \"Stop.\" The software will automatically process the videos and generate a Blender scene with the output data!</p> <p>To manually process/re-process the videos, use the <code>Process Motion Capture Videos</code> button in the <code>Processing</code> tab of the GUI.</p>"},{"location":"getting_started/single_camera_recording/","title":"Single-Camera Tutorial","text":""},{"location":"getting_started/single_camera_recording/#single-camera-recording","title":"Single-Camera Recording","text":""},{"location":"getting_started/single_camera_recording/#introduction","title":"Introduction","text":"<p>We recommend that everybody starts by creating a single-camera recording and reconstruction of their movement before moving on to more complex tasks like multi-camera calibration and reconstruction. </p>"},{"location":"getting_started/single_camera_recording/#installation","title":"Installation","text":"<p>Follow the Installation Guide to install the FreeMoCap software</p>"},{"location":"getting_started/single_camera_recording/#launching-freemocap","title":"Launching FreeMoCap","text":"<p>Launch FreeMoCap from the terminal by activating the relevant Python environment and typing <code>freemocap</code> into the terminal, then press Enter. At that point, the GUI should show up, which will look like this:</p> <p></p> <ul> <li>Click the friendly button that says \"Start a New Recording\" and then click the button that says \"Detect Cameras\" :</li> </ul>"},{"location":"getting_started/single_camera_recording/#camera-detection","title":"Camera Detection","text":"<p>The software should locate your cameras, and once they're connected, it will show a viewpoint from the connected camera in the GUI. You can adjust the settings in the sidebar and then click \"Apply Settings to Cameras\" to apply them. </p> <p>The exposure setting is your friend!</p> <p>The most important setting to look at right now is the exposure setting, which you should make as low as possible to decrease blur. We generally like to keep it below -6. Adjust it downwards until the image looks crisp, which will probably make it look slightly darker than you would normally expect. </p> <p>For this simple single-camera recording, this isn't a crucial step. As long as you can see yourself in the image, you should be tracked okay, but it's good to keep in mind for the future.</p>"},{"location":"getting_started/single_camera_recording/#recording","title":"Recording","text":"<p>Because you're doing a single camera recording, you don't need to do any calibration. But when you do graduate to multi-camera recordings, this is where you would get out a Charuco board and run a calibration first. We're all clear to record our motion capture for now though.</p> <p>Click \"Record\" and go into the field of view to perform some kind of movement. Then click \"Stop\", and it should process automatically from there. When it's done, it will pop up a Blender scene if Blender was properly detected and populate the folder with the output files!</p>"},{"location":"getting_started/software_hardware_prerequisites/","title":"Software hardware prerequisites","text":""},{"location":"getting_started/software_hardware_prerequisites/#prerequisites-for-using-freemocap","title":"Prerequisites for Using FreeMoCap","text":""},{"location":"getting_started/software_hardware_prerequisites/#1-required-equipment","title":"1. Required Equipment","text":"<p>The absolute minimum required equipment is a computer with a single camera on it. Even a simple laptop with a built-in camera can be used to create a single-camera recording. However, single-camera recordings will not produce reliable 3D estimates but will produce solid two-dimensional tracking, suitable for 2D animators. To get a viable multi-camera recording that will produce reliable estimates of 3D movement, users will need at least two cameras (one of which could be the laptop camera), however we recommend using three cameras for better results.</p> <p>These cameras should be connected directly to the computer's USB ports. We are working on support for using multiple cameras through a single USB hub, but for now, we recommend plugging the cameras straight into the computer for better results. For 3D reconstructions, users will also need to print out a Charuco board for the calibration process, which will be described in greater detail in the calibration section.</p> <p>Users may find it helpful to have USB extension cables and tripods to set up their webcams in a way that will allow them to get a good recording, but these are not necessary, just convenient.</p>"},{"location":"getting_started/software_hardware_prerequisites/#2-necessary-software","title":"2. Necessary Software","text":"<p>All of the packages needed for reconstructing movement data are included in the FreeMoCap software, with the exception of Blender, which is a free software that we recommend downloading from https://blender.org. Blender will be used at the end of the reconstruction process to create a Blender scene. If users want to explore the Jupyter notebooks generated with each recording, they'll need to install either VS Code or JupyterLab, and we can provide links to those resources.</p> <p>For now, the only way to install FreeMoCap is through a terminal, and we recommend using a virtual environment management software like Anaconda. Soon, we will include an option for downloading the software directly from the website. We'll have a section for the installation process and standard troubleshooting in another section.</p>"},{"location":"getting_started/your_first_recording/","title":"Your First Recording","text":"<p>Do the easy one first!</p> <p>We recommend that everybody starts with a single-camera reconstruction even if you intend to make multi-camera later.</p> <p>Single-camera recordings are MUCH simpler and faster than multi-camera recorings, and will help ensure that the full pipeline works on your computer with your hardware. </p> <p>Once that is established, you'll have a great baseline understanding of the compete process before moving to the more complex task of creating a multi-camera calibration and recording</p>"},{"location":"getting_started/your_first_recording/#single-camera-recording-recommended-for-first-timers","title":"Single Camera Recording (Recommended for first-timers)","text":""},{"location":"getting_started/your_first_recording/#multi-camera-calibration-recording","title":"Multi-Camera Calibration &amp; Recording","text":""},{"location":"resources/","title":"Resources","text":"<p>While FreeMoCap can record anywhere that's big enough to capture your entire body on video, some places and set ups will give better results than others. The tips below will help you configure your space to get the best recordings, whether you're working with high-end gear or simple webcams. </p> <p>Don't be afraid to start small and simple - any set up is better than no set up. You can always add complexity as you become more comfortable with the system.</p> <p>Grip it &amp; Rip it </p> <p>The best way to learn how this works it is to do it a lot!</p> <p>Use this section as a reference, but don't be afraid to just dive in and start recording! If things don't come out right, fiddle with your cameras and your settings and try it again! You don't have to understand how it works to use it, and you'll learn how it works by using it \u2728</p> <p>Start by recording A LOT of SHORT recordings, so that you can iterate quickly and get a feel for how the system works and the things the determine the quality of your output.</p> <p>If you get stuck, join our Discord and ask for help in the #help-requests channel!</p>"},{"location":"resources/#video-tutorial","title":"Video Tutorial","text":"<p>This video uses an older version of the software, but the discussion of hardware, lighting, and camera placement is still relevant (use the timestamps to jump to specific sections):</p>"},{"location":"resources/#detailed-setup-guide","title":"Detailed Setup Guide","text":""},{"location":"resources/#lighting-conditions","title":"Lighting Conditions","text":"<p>Lighting is crucial for a camera-based system like FreeMoCap. For best results, use bright environments, such as near open windows during the day. Be cautious, as environments that appear bright to our eyes may be quite dim (Human eyes are exceptionally good at adapting to different lighting conditions).</p> <p>Exposure Settings</p> <p>When setting the exposure on the GUI, aim for a setting of at most <code>-6</code>, but better results can be achieved at <code>-7</code> or <code>-8</code> or lower. </p> <p>On a technical level, we want enough light in our scene that we can set our camera's Exposure times to be as short as possible. </p> <p>A short exposure time means that the camera's (digital) shutter will be open for a shorter time on each recorded frame. That means that your subject will have less time to move during the exposure, which will reduce motion blur and make it easier to identify the subject's joints in each frame.</p> More on Exposure Settings <p>For information on how these camera exposure settings map onto actual time measurements, see this article.</p>"},{"location":"resources/#background-considerations","title":"Background Considerations","text":"<p>A solid-colored backdrop is not strictly necessary, but using one can improve the quality of recordings. Covering complex visual textures in the camera's field of view makes tracking the human in the scene easier, leading to better outcomes. Start with a simple setup first, and add complexity once you've established that the simple version works.</p> <p>Make Your Motion Capure Stand Out</p> <p>Having visual contrast between your subject and the background will help your subject stand out in recordings.</p> <p>Similarly, it is better for subjects to wear tight-fitting clothes that expose joints like elbows and knees. While not strictly necessary, it will lead to better quality recordings. Consider the machine's perspective: if the subject is wearing baggy pants and the software is asked to identify the knee, it will struggle. It may produce an estimate close to the correct position, but tracking the knee will be easier if it is visible or if the subject is wearing tight-fitting pants.</p>"},{"location":"resources/#camera-placement-and-configuration","title":"Camera Placement and Configuration","text":"<p>Camera placement is critical. Ensure that the subject is in full view of each camera. In multi-camera setups, ensure that at least two, preferably three, cameras can see every part of the subject's body at any given time. </p> <p>Working In A Tight Space</p> <p>Cameras can be rotated 90\u00b0 to portrait orientation to capture a standing human in a tighter space. You can even mix portrait and landscape views together to better capture your space.</p> <p>Ideally, the person should occupy as much of the camera screen as possible, providing more information for the software to track the person. Most testing has been done on low-quality webcams, although successful recordings have been made with GoPros, DSLRs, and mobile phones.</p> <p>In multi-camera situations, separate the cameras enough to provide different points of view on the subject. If multiple cameras are positioned too close together, they won't add much information to the scene. Separating them by a sufficient angle improves triangulation. Just make sure each camera can share a view of the Charuco board with another camera during calibration.</p> <p>High-quality recordings can be ensured by adjusting camera settings for optimal image results, mainly by setting the appropriate exposure for the lighting conditions and having good lighting. A successful calibration is also necessary, which you can read about in our Multi-Camera Calibration Tutorial.</p>"},{"location":"terminology/terminology/","title":"Terminology","text":""},{"location":"terminology/terminology/#terminology","title":"Terminology","text":""},{"location":"terminology/terminology/#capture-volume","title":"Capture Volume","text":"<p>3-dimensional area (volume) with sufficient camera coverage to support 3D tracking.</p>"},{"location":"terminology/terminology/#calibration","title":"Calibration","text":"<p>Link to a section of the 'braindump' video discussing capture volume calibration</p>"},{"location":"terminology/terminology/#charuco-board","title":"Charuco Board","text":"<p>Link to a section of the 'braindump' video discussing capture volume calibration</p>"},{"location":"terminology/terminology/#mediapipe","title":"Mediapipe","text":"<p>https://google.github.io/mediapipe/solutions/holistic</p>"},{"location":"terminology/terminology/#processing-stages","title":"Processing Stages","text":"<p>This is a brief description of each of the processing 'stages' necessary to use a bunch of USB webcams to reconstruct the 3D kinematic (i.e. mocap) data of the human subject! </p> <p>Some parts refer to the folder and function names of the <code>pre-alpha</code> version of the code, but the concepts are mostly the same in the <code>alpha</code> version.</p> <ul> <li>Stage 1 - Record Videos</li> <li>Record raw videos from attached USB webcams and timestamps for each frame </li> <li> <p>Raw Videos saved to <code>FreeMoCap_Data/[Session Folder]/RawVideos</code></p> </li> <li> <p>Stage 2 - Synchronize Videos</p> </li> <li>Use recorded timestamps to re-save raw videos as synchronized videos (same start and end and same number of frames). Videos saved to </li> <li> <p>Synchronized Videos saved to <code>FreeMoCap_Data/[Session Folder]/SynchedVideos</code></p> </li> <li> <p>Stage 3 - Calibrate Capture Volume</p> </li> <li>Use Anipose's Charuco-based calibration method to determine the location of each camera during a recording session and calibrate the capture volume</li> <li> <p>Calibration info saved to <code>[sessionID]_calibration.toml</code> and <code>[sessionID]_calibration.pickle</code> </p> </li> <li> <p>Stage 4 - Track 2D points in videos and Reconstruct 3D &lt;-This is where the magic happens \u2728</p> <ul> <li>Apply user specified tracking algorithms to Synchronized videos (currently supporting MediaPipe, OpenPose, and DeepLabCut) to generate 2D data <ul> <li>Save to <code>FreeMoCap_Data/[Session Folder]/DataArrays/</code> folder (e.g. <code>mediaPipeData_2d.npy</code>)</li> </ul> </li> <li>Combine 2d data from each camera with calibration data from Stage 3 to reconstruct the 3d trajectory of each tracked point<ul> <li>Save to <code>/DataArrays</code> folder (e.g. <code>openPoseSkel_3d.npy</code>)</li> </ul> </li> <li>NOTE - you might think it would make sense to separate the 2d tracking and 3d reconstruction into different stages, but the way the code is currently set up it's cleaner to combine them into the same processing stage \u00af\\_(\u30c4)_/\u00af</li> </ul> </li> <li> <p>Stage 5 - Use Blender to generate output data files (optional, requires Blender installed. set <code>freemocap.RunMe(useBlender=True)</code> to use)</p> <ul> <li>Hijack a user-installed version of Blender to format raw mocap data into  a <code>.blend</code> file including the raw data as keyframed emtpies with a (sloppy,  inexpertly) rigged and meshed armatured based on the Rigify Human Metarig</li> <li>Save <code>.blend</code> file to <code>[Session_Folder]/[Session_ID]/[Session_ID].blend</code> </li> <li>You can double click that <code>.blend</code> file to open it in Blender. </li> <li>For instructions on how to navigate a Blender Scene, try this YouTube Tutorial</li> </ul> </li> <li> <p>Stage 6 - Save Skeleton Animation!</p> <ul> <li>Create a Matplotlib based output animation video.</li> <li>Saves Animation video to: <code>[Session Folder]/[SessionID]_animVid.mp4</code></li> <li>Note - This part takes for-EVER \ud83d\ude05</li> </ul> </li> </ul>"},{"location":"terminology/terminology/#reprojection-error","title":"Reprojection Error","text":"<p>\"Reprojection error\" is the distance (in pixels?) between the originally measured point (i.e. the 2d skeleton) and the reconstructed 3d point reprojected back onto the image plane. </p> <p>The intuition is that if the 3d reconstruction and original 2d track are perfect, then reprojection error will be Zero. If it isn't, then there is some inaccurate in either:</p> <ul> <li>the original 2d tracks (i.e. bad skeleton detection in one or more cameras), </li> <li>in the 3d reconstruction (i.e. bad camera calibration), </li> <li>a combination of the two</li> </ul> <p>Click here to follow a conversation about reprojection error on discord</p>"},{"location":"troubleshooting/calibration_troubleshooting/","title":"Calibration Troubleshooting","text":"<p>Camera calibration is a smooth process once you get the hang of it, but it can take some trial and error to get your set up right. The following tips will help you smooth out the road bumps, and at the bottom is a list of common error messages you may see in the logging console, along with common solutions. </p> <p>If this guide isn't enough to get you calibrating your cameras successfully, reach out to us on our Discord to ask for more help.</p>"},{"location":"troubleshooting/calibration_troubleshooting/#charuco-board-size","title":"Charuco Board Size","text":"<p>Using a bigger charuco board can make it easier for your cameras to detect the board, especially in difficult lighting. Printing the board on a standard sheet of printer paper can work, but often bigger is better. Larger charuco boards can be printed directly on large poster board, or made by pasting together smaller partial printouts into a complete board.</p> <p>You can help make a small charuco board work by holding it closer to the cameras while recording, but make sure the board is still visible from multiple cameras at a time.</p>"},{"location":"troubleshooting/calibration_troubleshooting/#rigid-charuco-board","title":"Rigid Charuco Board","text":"<p>The software detecting the charuco is expecting the board to be perfectly flat. To calibrate properly, make sure your board is mounted to something rigid, like a piece of cardboard or poster board.</p>"},{"location":"troubleshooting/calibration_troubleshooting/#recording-length","title":"Recording Length","text":"<p>Taking time to record a longer calibration can help reduce your chances of a failed calibration. We recommend spending 5-10 seconds displaying the board to each pair of cameras. </p> <p>At 30fps, that should result in about 200 shared views of the board for each camera pair.</p>"},{"location":"troubleshooting/calibration_troubleshooting/#glare","title":"Glare","text":"<p>Glare from the sun or a birght light can obscure the charuco pattern and prevent the software from recognizing the charuco board. It can be helpful to tilt te board up and down while showing it to the cameras in order to ensure each camera has views of the board without glare.</p>"},{"location":"troubleshooting/calibration_troubleshooting/#missing-shared-views","title":"Missing Shared Views","text":"<p>Each camera must not only see the charuco board during calibration, but must share a view of the charuco board with another camera. It's best to avoid overly wide angles between cameras. As long as each camera is connected to each other camera by some combination of shared views with other cameras, the calibration can work.</p>"},{"location":"troubleshooting/calibration_troubleshooting/#reversed-images","title":"Reversed Images","text":"<p>Reversed or mirrored images, like those recorded from some front-facing cameras on mobile phones, will prevent the software from recognizing the charuco board. </p> <p>Some phones will allow you to turn off image mirroring in the settings, but if not you may have to switch to using the rear camera.</p>"},{"location":"troubleshooting/calibration_troubleshooting/#common-error-messages","title":"Common Error Messages","text":""},{"location":"troubleshooting/calibration_troubleshooting/#filenotfounderror-errno-2-no-such-file-or-directory","title":"<code>FileNotFoundError: [Errno 2] No such file or directory:</code>","text":"<p>This error occurs when no calibration file has been selected for the recording. If you are making a new recording, make sure you have recorded and processed calibration files for your current camera setup. If you already have a calibration for your camera setup, or are importing a recording that already has a calibration file, make sure to choose the \"Load calibration from file\" option in the \"Process Data\" tab, and load the TOML file that matches your recording.</p> <p></p>"},{"location":"troubleshooting/calibration_troubleshooting/#valueerror-not-enough-values-to-unpack-expected-2-got-0","title":"<code>ValueError: not enough values to unpack (expected 2, got 0)</code>","text":"<p>This issue comes up when one or more cameras do not have any shared views of the charuco with other cameras. This can due to the physical setup of your cameras. Make sure each camera can clearly see the charuco board at the same time as another camera. This issue can also happen if a camera is not properly detecting a charuco board due to an issue like a mirrored view, glare, or a charuco board that's too small in the cameras view.</p> <p>The full error message for this issue looks like this: </p><pre><code>freemocap_anipose.py\", line 1810, in calibrate_rows\n    objp, imgp = zip(*mixed)\n    ^^^^^^^^^^\nValueError: not enough values to unpack (expected 2, got 0)\n</code></pre>"},{"location":"troubleshooting/installation_troubleshooting/","title":"Installation Troubleshooting","text":"<p>We have instructions on installing FreeMoCap in our Installation Guide, but sometimes things don't go as smoothly as planned. This guide will help you work through common problems encountered while installing the software, and hopefully get FreeMoCap running in no time. At the bottom of the page is a list of common error messages you may see during installation, along with common solutions.</p> <p>If you're still having problems installing FreeMoCap after troubleshooting with the tips below, reach out to us on our Discord to ask for more help.</p>"},{"location":"troubleshooting/installation_troubleshooting/#use-a-new-environment","title":"Use a New Environment","text":"<p>Different python projects have different dendencies, and often those dependencies can clash with each other. If you have any installation problem, our first advice is to try installing and running FreeMoCap in a new, dedicated environment.  There's many options for creating and managing environments in Python - we recommend Poetry and Conda.</p>"},{"location":"troubleshooting/installation_troubleshooting/#check-your-python-version","title":"Check your Python Version","text":"<p>Currently, FreeMoCap works on Python versions 3.9 through 3.11.  We recommend using the most recent compatible version of Python. If installing FreeMoCap in a 3.11 environment doesn't work for you, then try it with a different python version and see if that helps.</p>"},{"location":"troubleshooting/installation_troubleshooting/#check-your-freemocap-version","title":"Check your FreeMoCap Version","text":"<p>FreeMoCap is under active development, and we try to address bugs as quickly as we can. It is always best to install the latest version of the software. If a prior installation isn't working, you can update versions by running <code>pip install freemocap --upgrade</code>. </p> <p>When pip begins installing the software, it will print which version it is installing. You can compare that to the most recent version listed on PyPi to make sure it is up to date.</p>"},{"location":"troubleshooting/installation_troubleshooting/#common-error-messages","title":"Common Error Messages","text":""},{"location":"troubleshooting/installation_troubleshooting/#command-freemocap-not-recognized","title":"<code>Command \"freemocap\" not recognized</code>","text":"<ul> <li>On some python installations, you may need to type <code>python -m freemocap</code> instead of just <code>freemocap</code> to launch the gui.</li> </ul>"}]}